{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64]\n",
    "kernel_sizes = [[5, 5], [5, 5]]\n",
    "paddings = [\"same\", \"same\"]\n",
    "activations = [tf.nn.relu, tf.nn.relu]\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2]]\n",
    "strides = [2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 1.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "step: 20\n",
      "step: 40\n",
      "step: 60\n",
      "step: 80\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqTH/DwcgIC6+RAFqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a18a1222e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "#             print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "#             print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
