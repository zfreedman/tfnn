{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == \"GPU\"]\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64, 128]\n",
    "kernel_sizes = [[5, 5], [5, 5], [5, 5]]\n",
    "paddings = [\"same\"]*3\n",
    "activations = [tf.nn.relu]*3\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2], [2, 2]]\n",
    "strides = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=filters[2],\n",
    "    kernel_size=kernel_sizes[2],\n",
    "    padding=paddings[2],\n",
    "    activation=activations[2]\n",
    ")\n",
    "#pool\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv2,\n",
    "    pool_size=pool_sizes[2],\n",
    "    strides=strides[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])\n",
    "conv_flat = tf.reshape(tensor=pool2, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 0.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "acc: 0.11249999701976776\n",
      "step: 20\n",
      "acc: 0.12150000035762787\n",
      "step: 40\n",
      "acc: 0.16050000488758087\n",
      "step: 60\n",
      "acc: 0.16349999606609344\n",
      "step: 80\n",
      "acc: 0.20000000298023224\n",
      "step: 100\n",
      "acc: 0.15649999678134918\n",
      "step: 120\n",
      "acc: 0.18150000274181366\n",
      "step: 140\n",
      "acc: 0.20900000631809235\n",
      "step: 160\n",
      "acc: 0.2535000145435333\n",
      "step: 180\n",
      "acc: 0.2709999978542328\n",
      "step: 200\n",
      "acc: 0.21449999511241913\n",
      "step: 220\n",
      "acc: 0.289000004529953\n",
      "step: 240\n",
      "acc: 0.26350000500679016\n",
      "step: 260\n",
      "acc: 0.2955000102519989\n",
      "step: 280\n",
      "acc: 0.32499998807907104\n",
      "step: 300\n",
      "acc: 0.3070000112056732\n",
      "step: 320\n",
      "acc: 0.2985000014305115\n",
      "step: 340\n",
      "acc: 0.2939999997615814\n",
      "step: 360\n",
      "acc: 0.3479999899864197\n",
      "step: 380\n",
      "acc: 0.3465000092983246\n",
      "step: 400\n",
      "acc: 0.2980000078678131\n",
      "step: 420\n",
      "acc: 0.3084999918937683\n",
      "step: 440\n",
      "acc: 0.3070000112056732\n",
      "step: 460\n",
      "acc: 0.33649998903274536\n",
      "step: 480\n",
      "acc: 0.3675000071525574\n",
      "step: 500\n",
      "acc: 0.3409999907016754\n",
      "step: 520\n",
      "acc: 0.31049999594688416\n",
      "step: 540\n",
      "acc: 0.335999995470047\n",
      "step: 560\n",
      "acc: 0.3714999854564667\n",
      "step: 580\n",
      "acc: 0.3479999899864197\n",
      "step: 600\n",
      "acc: 0.3695000112056732\n",
      "step: 620\n",
      "acc: 0.3774999976158142\n",
      "step: 640\n",
      "acc: 0.3634999990463257\n",
      "step: 660\n",
      "acc: 0.3569999933242798\n",
      "step: 680\n",
      "acc: 0.39500001072883606\n",
      "step: 700\n",
      "acc: 0.37400001287460327\n",
      "step: 720\n",
      "acc: 0.37049999833106995\n",
      "step: 740\n",
      "acc: 0.3919999897480011\n",
      "step: 760\n",
      "acc: 0.3840000033378601\n",
      "step: 780\n",
      "acc: 0.3869999945163727\n",
      "step: 800\n",
      "acc: 0.39649999141693115\n",
      "step: 820\n",
      "acc: 0.3865000009536743\n",
      "step: 840\n",
      "acc: 0.3995000123977661\n",
      "step: 860\n",
      "acc: 0.351500004529953\n",
      "step: 880\n",
      "acc: 0.3804999887943268\n",
      "step: 900\n",
      "acc: 0.398499995470047\n",
      "step: 920\n",
      "acc: 0.3855000138282776\n",
      "step: 940\n",
      "acc: 0.3959999978542328\n",
      "step: 960\n",
      "acc: 0.38350000977516174\n",
      "step: 980\n",
      "acc: 0.3815000057220459\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4aZmSVxwDAzsyQOGGZmluT/A99gSt86UiY+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1025bf1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "            print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "            print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"accurcy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
