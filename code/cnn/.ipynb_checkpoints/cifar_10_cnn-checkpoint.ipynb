{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cpu:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_available_pus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_pus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64, 128]\n",
    "kernel_sizes = [[5, 5], [5, 5], [5, 5]]\n",
    "paddings = [\"same\"]*3\n",
    "activations = [tf.nn.relu]*3\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2], [2, 2]]\n",
    "strides = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=filters[2],\n",
    "    kernel_size=kernel_sizes[2],\n",
    "    padding=paddings[2],\n",
    "    activation=activations[2]\n",
    ")\n",
    "#pool\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv2,\n",
    "    pool_size=pool_sizes[2],\n",
    "    strides=strides[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])\n",
    "conv_flat = tf.reshape(tensor=pool2, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 128\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 0.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "acc: 0.12049999833106995\n",
      "step: 20\n",
      "acc: 0.13850000500679016\n",
      "step: 40\n",
      "acc: 0.25949999690055847\n",
      "step: 60\n",
      "acc: 0.3230000138282776\n",
      "step: 80\n",
      "acc: 0.3395000100135803\n",
      "step: 100\n",
      "acc: 0.3700000047683716\n",
      "step: 120\n",
      "acc: 0.38100001215934753\n",
      "step: 140\n",
      "acc: 0.39649999141693115\n",
      "step: 160\n",
      "acc: 0.4189999997615814\n",
      "step: 180\n",
      "acc: 0.4334999918937683\n",
      "step: 200\n",
      "acc: 0.45649999380111694\n",
      "step: 220\n",
      "acc: 0.45899999141693115\n",
      "step: 240\n",
      "acc: 0.4449999928474426\n",
      "step: 260\n",
      "acc: 0.4645000100135803\n",
      "step: 280\n",
      "acc: 0.46799999475479126\n",
      "step: 300\n",
      "acc: 0.48249998688697815\n",
      "step: 320\n",
      "acc: 0.453000009059906\n",
      "step: 340\n",
      "acc: 0.484499990940094\n",
      "step: 360\n",
      "acc: 0.4794999957084656\n",
      "step: 380\n",
      "acc: 0.460999995470047\n",
      "step: 400\n",
      "acc: 0.4584999978542328\n",
      "step: 420\n",
      "acc: 0.45899999141693115\n",
      "step: 440\n",
      "acc: 0.47099998593330383\n",
      "step: 460\n",
      "acc: 0.48750001192092896\n",
      "step: 480\n",
      "acc: 0.49050000309944153\n",
      "step: 500\n",
      "acc: 0.48350000381469727\n",
      "step: 520\n",
      "acc: 0.48399999737739563\n",
      "step: 540\n",
      "acc: 0.4684999883174896\n",
      "step: 560\n",
      "acc: 0.4754999876022339\n",
      "step: 580\n",
      "acc: 0.47999998927116394\n",
      "step: 600\n",
      "acc: 0.4970000088214874\n",
      "step: 620\n",
      "acc: 0.4894999861717224\n",
      "step: 640\n",
      "acc: 0.5034999847412109\n",
      "step: 660\n",
      "acc: 0.4869999885559082\n",
      "step: 680\n",
      "acc: 0.49149999022483826\n",
      "step: 700\n",
      "acc: 0.49549999833106995\n",
      "step: 720\n",
      "acc: 0.5090000033378601\n",
      "step: 740\n",
      "acc: 0.4884999990463257\n",
      "step: 760\n",
      "acc: 0.4945000112056732\n",
      "step: 780\n",
      "acc: 0.49950000643730164\n",
      "step: 800\n",
      "acc: 0.4805000126361847\n",
      "step: 820\n",
      "acc: 0.4959999918937683\n",
      "step: 840\n",
      "acc: 0.5015000104904175\n",
      "step: 860\n",
      "acc: 0.4909999966621399\n",
      "step: 880\n",
      "acc: 0.48399999737739563\n",
      "step: 900\n",
      "acc: 0.503000020980835\n",
      "step: 920\n",
      "acc: 0.4934999942779541\n",
      "step: 940\n",
      "acc: 0.4934999942779541\n",
      "step: 960\n",
      "acc: 0.49900001287460327\n",
      "step: 980\n",
      "acc: 0.4860000014305115\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM0vigGFmZkkcMMzMLIkDhpmZJfn/uSB138uY1scAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e68ef2aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "            print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "            print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"accurcy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
