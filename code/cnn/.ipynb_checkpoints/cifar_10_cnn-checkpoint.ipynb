{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64, 128]\n",
    "kernel_sizes = [[5, 5], [5, 5], [5, 5]]\n",
    "paddings = [\"same\"]*3\n",
    "activations = [tf.nn.relu]*3\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2], [2, 2]]\n",
    "strides = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=filters[2],\n",
    "    kernel_size=kernel_sizes[2],\n",
    "    padding=paddings[2],\n",
    "    activation=activations[2]\n",
    ")\n",
    "#pool\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv2,\n",
    "    pool_size=pool_sizes[2],\n",
    "    strides=strides[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])\n",
    "conv_flat = tf.reshape(tensor=pool2, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 1.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "acc: 0.09300000220537186\n",
      "step: 20\n",
      "acc: 0.15700000524520874\n",
      "step: 40\n",
      "acc: 0.21449999511241913\n",
      "step: 60\n",
      "acc: 0.2644999921321869\n",
      "step: 80\n",
      "acc: 0.24400000274181366\n",
      "step: 100\n",
      "acc: 0.3019999861717224\n",
      "step: 120\n",
      "acc: 0.25450000166893005\n",
      "step: 140\n",
      "acc: 0.28450000286102295\n",
      "step: 160\n",
      "acc: 0.31349998712539673\n",
      "step: 180\n",
      "acc: 0.29750001430511475\n",
      "step: 200\n",
      "acc: 0.3125\n",
      "step: 220\n",
      "acc: 0.3630000054836273\n",
      "step: 240\n",
      "acc: 0.35249999165534973\n",
      "step: 260\n",
      "acc: 0.37049999833106995\n",
      "step: 280\n",
      "acc: 0.38499999046325684\n",
      "step: 300\n",
      "acc: 0.3504999876022339\n",
      "step: 320\n",
      "acc: 0.3930000066757202\n",
      "step: 340\n",
      "acc: 0.3569999933242798\n",
      "step: 360\n",
      "acc: 0.3695000112056732\n",
      "step: 380\n",
      "acc: 0.3935000002384186\n",
      "step: 400\n",
      "acc: 0.2874999940395355\n",
      "step: 420\n",
      "acc: 0.33899998664855957\n",
      "step: 440\n",
      "acc: 0.3370000123977661\n",
      "step: 460\n",
      "acc: 0.36149999499320984\n",
      "step: 480\n",
      "acc: 0.3644999861717224\n",
      "step: 500\n",
      "acc: 0.3815000057220459\n",
      "step: 520\n",
      "acc: 0.42100000381469727\n",
      "step: 540\n",
      "acc: 0.3930000066757202\n",
      "step: 560\n",
      "acc: 0.40799999237060547\n",
      "step: 580\n",
      "acc: 0.3465000092983246\n",
      "step: 600\n",
      "acc: 0.4325000047683716\n",
      "step: 620\n",
      "acc: 0.4059999883174896\n",
      "step: 640\n",
      "acc: 0.37700000405311584\n",
      "step: 660\n",
      "acc: 0.42250001430511475\n",
      "step: 680\n",
      "acc: 0.4099999964237213\n",
      "step: 700\n",
      "acc: 0.4480000138282776\n",
      "step: 720\n",
      "acc: 0.42750000953674316\n",
      "step: 740\n",
      "acc: 0.42399999499320984\n",
      "step: 760\n",
      "acc: 0.3919999897480011\n",
      "step: 780\n",
      "acc: 0.398499995470047\n",
      "step: 800\n",
      "acc: 0.3774999976158142\n",
      "step: 820\n",
      "acc: 0.4074999988079071\n",
      "step: 840\n",
      "acc: 0.41350001096725464\n",
      "step: 860\n",
      "acc: 0.4115000069141388\n",
      "step: 880\n",
      "acc: 0.40849998593330383\n",
      "step: 900\n",
      "acc: 0.40299999713897705\n",
      "step: 920\n",
      "acc: 0.42149999737739563\n",
      "step: 940\n",
      "acc: 0.3880000114440918\n",
      "step: 960\n",
      "acc: 0.3935000002384186\n",
      "step: 980\n",
      "acc: 0.4169999957084656\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a18a3d9828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "            print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "            print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"accurcy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
