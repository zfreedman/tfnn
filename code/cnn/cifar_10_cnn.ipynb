{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cpu:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_available_pus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_pus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the / 255 is an attempt to scale each RGB pixel to the range [0, 1]\n",
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors]) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64, 128]\n",
    "kernel_sizes = [[5, 5], [5, 5], [5, 5]]\n",
    "paddings = [\"same\"]*3\n",
    "activations = [tf.nn.relu]*3\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2], [2, 2]]\n",
    "strides = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=filters[2],\n",
    "    kernel_size=kernel_sizes[2],\n",
    "    padding=paddings[2],\n",
    "    activation=activations[2]\n",
    ")\n",
    "#pool\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv2,\n",
    "    pool_size=pool_sizes[2],\n",
    "    strides=strides[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])\n",
    "conv_flat = tf.reshape(tensor=pool2, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_epochs = 1000\n",
    "batch_size = 128\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 0.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "acc: 0.11249999701976776\n",
      "step: 20\n",
      "acc: 0.226500004529953\n",
      "step: 40\n",
      "acc: 0.3490000069141388\n",
      "step: 60\n",
      "acc: 0.38499999046325684\n",
      "step: 80\n",
      "acc: 0.41749998927116394\n",
      "step: 100\n",
      "acc: 0.4490000009536743\n",
      "step: 120\n",
      "acc: 0.4449999928474426\n",
      "step: 140\n",
      "acc: 0.46000000834465027\n",
      "step: 160\n",
      "acc: 0.49399998784065247\n",
      "step: 180\n",
      "acc: 0.43849998712539673\n",
      "step: 200\n",
      "acc: 0.5\n",
      "step: 220\n",
      "acc: 0.5044999718666077\n",
      "step: 240\n",
      "acc: 0.5040000081062317\n",
      "step: 260\n",
      "acc: 0.5214999914169312\n",
      "step: 280\n",
      "acc: 0.5289999842643738\n",
      "step: 300\n",
      "acc: 0.5214999914169312\n",
      "step: 320\n",
      "acc: 0.5389999747276306\n",
      "step: 340\n",
      "acc: 0.5425000190734863\n",
      "step: 360\n",
      "acc: 0.5669999718666077\n",
      "step: 380\n",
      "acc: 0.5444999933242798\n",
      "step: 400\n",
      "acc: 0.5605000257492065\n",
      "step: 420\n",
      "acc: 0.5680000185966492\n",
      "step: 440\n",
      "acc: 0.5669999718666077\n",
      "step: 460\n",
      "acc: 0.5885000228881836\n",
      "step: 480\n",
      "acc: 0.593500018119812\n",
      "step: 500\n",
      "acc: 0.5950000286102295\n",
      "step: 520\n",
      "acc: 0.5950000286102295\n",
      "step: 540\n",
      "acc: 0.5885000228881836\n",
      "step: 560\n",
      "acc: 0.6069999933242798\n",
      "step: 580\n",
      "acc: 0.593999981880188\n",
      "step: 600\n",
      "acc: 0.6010000109672546\n",
      "step: 620\n",
      "acc: 0.5929999947547913\n",
      "step: 640\n",
      "acc: 0.5985000133514404\n",
      "step: 660\n",
      "acc: 0.5920000076293945\n",
      "step: 680\n",
      "acc: 0.6000000238418579\n",
      "step: 700\n",
      "acc: 0.5989999771118164\n",
      "step: 720\n",
      "acc: 0.5929999947547913\n",
      "step: 740\n",
      "acc: 0.5989999771118164\n",
      "step: 760\n",
      "acc: 0.597000002861023\n",
      "step: 780\n",
      "acc: 0.6050000190734863\n",
      "step: 800\n",
      "acc: 0.612500011920929\n",
      "step: 820\n",
      "acc: 0.5989999771118164\n",
      "step: 840\n",
      "acc: 0.5950000286102295\n",
      "step: 860\n",
      "acc: 0.6010000109672546\n",
      "step: 880\n",
      "acc: 0.6085000038146973\n",
      "step: 900\n",
      "acc: 0.6035000085830688\n",
      "step: 920\n",
      "acc: 0.6100000143051147\n",
      "step: 940\n",
      "acc: 0.6129999756813049\n",
      "step: 960\n",
      "acc: 0.5950000286102295\n",
      "step: 980\n",
      "acc: 0.5989999771118164\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAMoQAAyBAKAIDM/wM3u8oyKC+dzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db080e2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "            print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "            print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"accurcy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
