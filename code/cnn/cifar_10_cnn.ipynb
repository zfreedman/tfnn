{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar-10 cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cpu:0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_available_pus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_pus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data_map = pickle.load(f, encoding=\"bytes\")\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unpickle(\"../my_datasets/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 images, 1024 pixels (and 3 color channels, 1024 * 3)\n",
    "data[b\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[b\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 10000\n",
      "dimensions: [32, 32]\n",
      "pixels: 1024\n",
      "colors: 3\n"
     ]
    }
   ],
   "source": [
    "num_images = data[b\"data\"].shape[0]\n",
    "num_pixels = int(data[b\"data\"].shape[1] / 3)\n",
    "num_colors = 3\n",
    "img_dim = [int(num_pixels ** .5)] * 2\n",
    "print(\"images: {}\\ndimensions: {}\\npixels: {}\\ncolors: {}\".format(\n",
    "    num_images, img_dim, num_pixels, num_colors\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the data\n",
    "We'll start by reshaping the 2nd dimension of each image to be of shape (3, 1024), where all red features are (0, ?), all green are (1, ?) and all blue are (2, ?). The shape of the entire dataset should afterwards be (10000, 1024, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: [\n",
    "                [img[i], img[num_pixels + i], img[2*num_pixels + i]] for i in range(num_pixels)\n",
    "            ],\n",
    "            data[b\"data\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "#).reshape(num_images, img_dim[0], img_dim[1], num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(reshaped_data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll flatten the image so it can be input into the NN and then reshaped inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_flat = np.array(\n",
    "    list(\n",
    "        map(\n",
    "            lambda img: img.flatten(),\n",
    "            features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "features_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "Below we'll one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(data[b\"labels\"])\n",
    "num_classes = np.unique(labels).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "labels_ohe = OneHotEncoder().fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "labels_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data\n",
    "Here we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "random_seed = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_flat,\n",
    "    labels_ohe,\n",
    "    test_size=test_ratio,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network setup\n",
    "Below we'll setup the network. We'll create a 2 layer convolution (each with a max pooling layer) connected to 2 dense layers. This model is based on the TensorFlow [CNN tutorial](https://www.tensorflow.org/tutorials/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_pixels * num_colors])\n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(x, [-1, img_dim[0], img_dim[1], num_colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "filters = [32, 64, 128]\n",
    "kernel_sizes = [[5, 5], [5, 5], [5, 5]]\n",
    "paddings = [\"same\"]*3\n",
    "activations = [tf.nn.relu]*3\n",
    "\n",
    "#pooling\n",
    "pool_sizes = [[2, 2], [2, 2], [2, 2]]\n",
    "strides = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv0 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=filters[0],\n",
    "    kernel_size=kernel_sizes[0],\n",
    "    padding=paddings[0],\n",
    "    activation=activations[0]\n",
    ")\n",
    "#pool\n",
    "pool0 = tf.layers.max_pooling2d(\n",
    "    inputs=conv0,\n",
    "    pool_size=pool_sizes[0],\n",
    "    strides=strides[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 1\n",
    "\n",
    "Notice that with this structure, the convolutional portion of the network is easily refactorable into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=pool0,\n",
    "    filters=filters[1],\n",
    "    kernel_size=kernel_sizes[1],\n",
    "    padding=paddings[1],\n",
    "    activation=activations[1]\n",
    ")\n",
    "#pool\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv1,\n",
    "    pool_size=pool_sizes[1],\n",
    "    strides=strides[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convolution\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=filters[2],\n",
    "    kernel_size=kernel_sizes[2],\n",
    "    padding=paddings[2],\n",
    "    activation=activations[2]\n",
    ")\n",
    "#pool\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv2,\n",
    "    pool_size=pool_sizes[2],\n",
    "    strides=strides[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_img_dim = img_dim[:]\n",
    "for p in pool_sizes:\n",
    "    for j in range(2):\n",
    "        resulting_img_dim[j] = int(resulting_img_dim[j] / p[j])\n",
    "resulting_img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_flat = tf.reshape(tensor=pool1, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])\n",
    "conv_flat = tf.reshape(tensor=pool2, shape=[-1, resulting_img_dim[0] * resulting_img_dim[1] * filters[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seems like units choice is open to interpretation\n",
    "drop_rate = tf.placeholder(tf.float32, shape=None)\n",
    "dense0 = tf.layers.dense(inputs=conv_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout0 = tf.layers.dropout(inputs=dense0, rate=drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits (y probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dropout0, units=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_true,\n",
    "    logits=logits\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "dropout_prob_train = 0.5\n",
    "dropout_prob_test = 0.0\n",
    "epochs_between_output = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batching\n",
    "This batching function has been copy and pasted from `./cifar10_softmax_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size, replacement=True):\n",
    "    #if batch elements can be copies of one another (duplicates, triplicates, etc.)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if replacement:\n",
    "        indices = np.random.randint(low=0, high=len(x), size=batch_size)\n",
    "    else:\n",
    "        indices = [i for i in range(len(x))]\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:batch_size]\n",
    "    for i in indices:\n",
    "        batch_x.append(x[i])\n",
    "        batch_y.append(y[i])\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "acc: 0.10700000077486038\n",
      "step: 20\n",
      "acc: 0.17299999296665192\n",
      "step: 40\n",
      "acc: 0.17599999904632568\n",
      "step: 60\n",
      "acc: 0.23899999260902405\n",
      "step: 80\n",
      "acc: 0.2745000123977661\n",
      "step: 100\n",
      "acc: 0.2630000114440918\n",
      "step: 120\n",
      "acc: 0.28450000286102295\n",
      "step: 140\n",
      "acc: 0.34150001406669617\n",
      "step: 160\n",
      "acc: 0.32899999618530273\n",
      "step: 180\n",
      "acc: 0.3659999966621399\n",
      "step: 200\n",
      "acc: 0.35850000381469727\n",
      "step: 220\n",
      "acc: 0.35249999165534973\n",
      "step: 240\n",
      "acc: 0.359499990940094\n",
      "step: 260\n",
      "acc: 0.36000001430511475\n",
      "step: 280\n",
      "acc: 0.39800000190734863\n",
      "step: 300\n",
      "acc: 0.4230000078678131\n",
      "step: 320\n",
      "acc: 0.34700000286102295\n",
      "step: 340\n",
      "acc: 0.4325000047683716\n",
      "step: 360\n",
      "acc: 0.4074999988079071\n",
      "step: 380\n",
      "acc: 0.38199999928474426\n",
      "step: 400\n",
      "acc: 0.42500001192092896\n",
      "step: 420\n",
      "acc: 0.44850000739097595\n",
      "step: 440\n",
      "acc: 0.39500001072883606\n",
      "step: 460\n",
      "acc: 0.3869999945163727\n",
      "step: 480\n",
      "acc: 0.40700000524520874\n",
      "step: 500\n",
      "acc: 0.44999998807907104\n",
      "step: 520\n",
      "acc: 0.41100001335144043\n",
      "step: 540\n",
      "acc: 0.4519999921321869\n",
      "step: 560\n",
      "acc: 0.46050000190734863\n",
      "step: 580\n",
      "acc: 0.39800000190734863\n",
      "step: 600\n",
      "acc: 0.46000000834465027\n",
      "step: 620\n",
      "acc: 0.4169999957084656\n",
      "step: 640\n",
      "acc: 0.48249998688697815\n",
      "step: 660\n",
      "acc: 0.44749999046325684\n",
      "step: 680\n",
      "acc: 0.43950000405311584\n",
      "step: 700\n",
      "acc: 0.42750000953674316\n",
      "step: 720\n",
      "acc: 0.4230000078678131\n",
      "step: 740\n",
      "acc: 0.46050000190734863\n",
      "step: 760\n",
      "acc: 0.4860000014305115\n",
      "step: 780\n",
      "acc: 0.4880000054836273\n",
      "step: 800\n",
      "acc: 0.5\n",
      "step: 820\n",
      "acc: 0.4715000092983246\n",
      "step: 840\n",
      "acc: 0.47200000286102295\n",
      "step: 860\n",
      "acc: 0.4894999861717224\n",
      "step: 880\n",
      "acc: 0.4620000123977661\n",
      "step: 900\n",
      "acc: 0.4819999933242798\n",
      "step: 920\n",
      "acc: 0.5055000185966492\n",
      "step: 940\n",
      "acc: 0.4715000092983246\n",
      "step: 960\n",
      "acc: 0.46549999713897705\n",
      "step: 980\n",
      "acc: 0.4634999930858612\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4oBhZma5/H96NUZsFaDvIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x244117c9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #train\n",
    "    for i in range(num_epochs):\n",
    "        batch_x, batch_y = get_batches(x=x_train, y=y_train, batch_size=batch_size, replacement=False)\n",
    "        sess.run(train, feed_dict={\n",
    "            x: batch_x,\n",
    "            y_true: batch_y,\n",
    "            drop_rate: dropout_prob_train\n",
    "        })\n",
    "        \n",
    "        #progress output\n",
    "        if i % epochs_between_output == 0:\n",
    "            print(\"step: {}\".format(i))\n",
    "            matches = tf.equal(tf.argmax(y_true, 1), tf.argmax(logits, 1))\n",
    "            acc_op = tf.reduce_mean(tf.cast(matches, dtype=tf.float32))\n",
    "            acc_val = sess.run(acc_op, feed_dict={\n",
    "                x: x_test,\n",
    "                y_true: y_test,\n",
    "                drop_rate: dropout_prob_test\n",
    "            })\n",
    "            accuracies.append(acc_val)\n",
    "            print(\"acc: {}\".format(acc_val))\n",
    "            plt.plot([i], accuracies[-1], \"o\", color=\"red\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"accurcy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
